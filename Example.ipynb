{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of how to use `DiscriminationMitigator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from DiscriminationMitigation import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_synth(n=10000, css_probab=0.5, gamma0=4, gamma1=6, alpha0=2, alpha1=1, beta0=1, beta1=1):\n",
    "\n",
    "    np.random.seed(123)\n",    "\n",
    "    # Protected class variable\n",
    "    c1 = np.random.binomial(1, p=class_probab, size=n) # group 1\n",
    "    c0 = 1-c1 # group 0\n",
    "\n",
    "    # Other covariate\n",
    "    w = gamma0*c0 + gamma1*c1 + np.random.normal(0, 0.5, size=n) # linear function of class & shock\n",
    "\n",
    "    # Outcome variable\n",
    "    y = alpha0*c0 + alpha1*c1 + beta0*c0*w + beta1*c1*w + np.random.normal(0, 0.5, size=n)\n",
    "\n",
    "    return pd.DataFrame([y, c0, c1, w]).T.rename(columns={0:'y', 1: 'c0', 2: 'c1', 3: 'w'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate some synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          y   c0   c1         w  z  a   b   c\n",
      "0  7.383773  0.0  1.0  6.479200  2  1  10  18\n",
      "1  6.255114  1.0  0.0  4.230080  3  1  11  14\n",
      "2  5.614841  1.0  0.0  3.773609  4  1   4  19\n",
      "3  7.692184  0.0  1.0  6.553467  4  1  13  19\n",
      "4  7.440835  0.0  1.0  6.139432  1  1  12  19\n",
      "\n",
      " (10000, 8)\n"
     ]
    }
   ],
   "source": [
    "synth = simple_synth()\n",
    "synth['z'] = np.random.randint(low=1, high=5, size=len(synth)) # add higher-dimensional protected class\n",
    "synth['a'] = np.random.randint(low=1, high=2, size=len(synth)) # other random, uncorrelated features\n",
    "synth['b'] = np.random.randint(low=1, high=15, size=len(synth))\n",
    "synth['c'] = np.random.randint(low=5, high=20, size=len(synth))\n",
    "print(synth.head())\n",
    "print(\"\\n\", synth.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get example configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example configuration dictionary: \n",
      " {'protected_class_features': ['c0', 'c1', 'z'], 'target_feature': ['y']}\n"
     ]
    }
   ],
   "source": [
    "with open('example_config.json') as j:\n",
    "    config = json.load(j)\n",
    "\n",
    "print(\"Example configuration dictionary: \\n\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get example marginal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'z': {'1': 0.9, '2': 0.02, '3': 0.04, '4': 0.04}}\n"
     ]
    }
   ],
   "source": [
    "with open('example_weights.json') as j:\n",
    "    weights = json.load(j)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `weights` allows users to supply a dictionary of custom marginal distributions for each protected class feature. Suppose you'd like to put more weight on predictions that treated individuals as though they wee members of value 1 in protected class feature 'z' -- say, if the weight on z=1 were 0.9, rather than ~0.25? You'd simply need to change the values in the weights dictionary. Importantly marginals per feature must sum to 1. Further, in this example, 'c0' and 'c1' are one-hot vectors for a binary random variable. In this case, if you altered the share of one group in 'c0' you'd also need to apply the inverse to 'c1' so that the marginals reflected mutual exclusivity. If `DiscriminationMitigator` detects two variables as possible one-hot vectors (i.e. they are extremely correlated), it will raise a Warning, but it *will not* enforce that the marginals of adjacent one-hot vectors are indeed inverses.\n",
    "\n",
    "Additionally, JSON files require keys (in this case, feature names) to be strings, so make sure yours are. `DiscriminationMitigator` converts these to their correct numeric format, so you need not worry about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7600, 7)\n",
      "(1900, 7)\n",
      "(500, 7)\n"
     ]
    }
   ],
   "source": [
    "# Train (and val) / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(synth.loc[:, ~synth.columns.isin(config['target_feature'])],\n",
    "                                                    synth[config['target_feature']], random_state=123,\n",
    "                                                    test_size=500)\n",
    "\n",
    "# Train / val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=123, test_size=0.2)\n",
    "\n",
    "for x in X_train, X_val, X_test:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a Tensorflow Keras Sequential deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 64        \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 225\n",
      "Trainable params: 225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=7,))\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(16))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 16.4928 - val_loss: 2.2919\n",
      "Epoch 2/10\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 7.2325 - val_loss: 1.3808\n",
      "Epoch 3/10\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 5.0945 - val_loss: 0.9383\n",
      "Epoch 4/10\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 3.8912 - val_loss: 0.6666\n",
      "Epoch 5/10\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 3.3055 - val_loss: 0.6422\n",
      "Epoch 6/10\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 2.8391 - val_loss: 0.6187\n",
      "Epoch 7/10\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 2.5244 - val_loss: 0.3920\n",
      "Epoch 8/10\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 2.2051 - val_loss: 0.4121\n",
      "Epoch 9/10\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 2.0020 - val_loss: 0.3747\n",
      "Epoch 10/10\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 1.7945 - val_loss: 0.3905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x196f6a8fc70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrimination mitigation tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Use just the marginals from `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = DiscriminationMitigator(df=X_test, model=model, config=config, train=None, weights=None).predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following examples, unadjusted predictions (`unadj_pred`) and uniform weights (`unif_wts`) remain the same. Population weights (`pop_wts`) in this case pertain to the marginal distributions per protected class feature in `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe of predictions: \n",
      "    unadj_pred  unif_wts   pop_wts\n",
      "0    6.746980  6.758377  6.767809\n",
      "1    6.075054  6.103145  6.107012\n",
      "2    5.917782  5.889691  5.885960\n",
      "3    6.499167  6.521694  6.527415\n",
      "4    6.741848  6.753245  6.762677\n",
      "\n",
      "Statistical moments: \n",
      "        unadj_pred    unif_wts     pop_wts\n",
      "count  500.000000  500.000000  500.000000\n",
      "mean     6.191450    6.191427    6.191450\n",
      "std      0.549065    0.565770    0.571420\n",
      "min      4.960387    4.943426    4.935985\n",
      "25%      5.721015    5.703740    5.695928\n",
      "50%      6.198308    6.196548    6.198081\n",
      "75%      6.657816    6.679993    6.685009\n",
      "max      7.633538    7.650499    7.658076\n",
      "\n",
      "Correlation matrix of predictions: \n",
      "             unadj_pred  unif_wts   pop_wts\n",
      "unadj_pred    1.000000  0.999761  0.999663\n",
      "unif_wts      0.999761  1.000000  0.999976\n",
      "pop_wts       0.999663  0.999976  1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataframe of predictions: \\n\", ex1.reset_index(drop=True).head())\n",
    "print(\"\\nStatistical moments: \\n\", ex1.describe())\n",
    "print(\"\\nCorrelation matrix of predictions: \\n\", ex1.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Use the marginals from another dataset (`train`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2 = DiscriminationMitigator(df=X_test, model=model, config=config, train=X_train, weights=None).predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases that the training set (or another dataset) is considerably larger and potentially more population representative than `df`, you may want to reweight the adjusted predictions in `pop_wts` to the marginals of all protected class features in this other dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe of predictions: \n",
      "    unadj_pred  unif_wts   pop_wts\n",
      "0    6.746980  6.758377  6.767430\n",
      "1    6.075054  6.103145  6.106634\n",
      "2    5.917782  5.889691  5.885582\n",
      "3    6.499167  6.521694  6.527037\n",
      "4    6.741848  6.753245  6.762299\n",
      "\n",
      "Statistical moments: \n",
      "        unadj_pred    unif_wts     pop_wts\n",
      "count  500.000000  500.000000  500.000000\n",
      "mean     6.191450    6.191427    6.191072\n",
      "std      0.549065    0.565770    0.571420\n",
      "min      4.960387    4.943426    4.935606\n",
      "25%      5.721015    5.703740    5.695550\n",
      "50%      6.198308    6.196548    6.197703\n",
      "75%      6.657816    6.679993    6.684631\n",
      "max      7.633538    7.650499    7.657698\n",
      "\n",
      "Correlation matrix of predictions: \n",
      "             unadj_pred  unif_wts   pop_wts\n",
      "unadj_pred    1.000000  0.999761  0.999663\n",
      "unif_wts      0.999761  1.000000  0.999976\n",
      "pop_wts       0.999663  0.999976  1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataframe of predictions: \\n\", ex2.reset_index(drop=True).head())\n",
    "print(\"\\nStatistical moments: \\n\", ex2.describe())\n",
    "print(\"\\nCorrelation matrix of predictions: \\n\", ex2.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare population weights from 'df' vs. 'train':\n",
      "               df       train\n",
      "count  500.000000  500.000000\n",
      "mean     6.191450    6.191072\n",
      "std      0.571420    0.571420\n",
      "min      4.935985    4.935606\n",
      "25%      5.695928    5.695550\n",
      "50%      6.198081    6.197703\n",
      "75%      6.685009    6.684631\n",
      "max      7.658076    7.657698\n"
     ]
    }
   ],
   "source": [
    "compare_pop = pd.concat([ex1['pop_wts'].rename('df'), ex2['pop_wts'].rename('train')], axis=1)\n",
    "print(\"Compare population weights from 'df' vs. 'train':\")\n",
    "print(compare_pop.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: Use the marginals from another dataset and use custom weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning! The following features are extremely correlated and thus may be one-hot vectors: c0 c1. \n",
      "If no category is omitted, users must ensure custom marginal weights for one-hot vectors align correctly.\n"
     ]
    }
   ],
   "source": [
    "ex3 = DiscriminationMitigator(df=X_test, model=model, config=config, train=X_train, weights=weights).predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also want to reweight predictions to ask 'what-if' questions: i.e. what if the share of group *x* were different than their observed share in the data? Providing a dictionary of marginal distributions to `weights` will allow for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom weights: {'z': {'1': 0.9, '2': 0.02, '3': 0.04, '4': 0.04}} \n",
      "\n",
      "Dataframe of predictions: \n",
      "    unadj_pred  unif_wts   pop_wts  cust_wts\n",
      "0    6.746980  6.758377  6.767430  6.744531\n",
      "1    6.075054  6.103145  6.106634  6.105993\n",
      "2    5.917782  5.889691  5.885582  5.915334\n",
      "3    6.499167  6.521694  6.527037  6.518977\n",
      "4    6.741848  6.753245  6.762299  6.739400\n",
      "\n",
      "Statistical moments: \n",
      "        unadj_pred    unif_wts     pop_wts    cust_wts\n",
      "count  500.000000  500.000000  500.000000  500.000000\n",
      "mean     6.191450    6.191427    6.191072    6.205807\n",
      "std      0.549065    0.565770    0.571420    0.548989\n",
      "min      4.960387    4.943426    4.935606    4.980197\n",
      "25%      5.721015    5.703740    5.695550    5.741638\n",
      "50%      6.198308    6.196548    6.197703    6.206755\n",
      "75%      6.657816    6.679993    6.684631    6.681364\n",
      "max      7.633538    7.650499    7.657698    7.642218\n",
      "\n",
      "Correlation matrix of predictions: \n",
      "             unadj_pred  unif_wts   pop_wts  cust_wts\n",
      "unadj_pred    1.000000  0.999761  0.999663  0.999761\n",
      "unif_wts      0.999761  1.000000  0.999976  0.999768\n",
      "pop_wts       0.999663  0.999976  1.000000  0.999596\n",
      "cust_wts      0.999761  0.999768  0.999596  1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Custom weights:\", weights, \"\\n\")\n",
    "print(\"Dataframe of predictions: \\n\", ex3.reset_index(drop=True).head())\n",
    "print(\"\\nStatistical moments: \\n\", ex3.describe())\n",
    "print(\"\\nCorrelation matrix of predictions: \\n\", ex3.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 4: Reweighting multiple features at onece - possible, but to be avoided!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning! The following features are extremely correlated and thus may be one-hot vectors: c0 c1. \n",
      "If no category is omitted, users must ensure custom marginal weights for one-hot vectors align correctly.\n"
     ]
    }
   ],
   "source": [
    "new_weights = {'c0': {0.0: 0.1, 1.0: 0.9}, 'c1': {0.0: 0.9, 1.0: 0.1}, 'z': {1.0: 0.9, 2.0: 0.02, 3.0: 0.04, 4.0: 0.04}}\n",
    "ex4 = DiscriminationMitigator(df=X_test, model=model, config=config, train=X_train, weights=new_weights).predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JOE: I don't understand what you are getting at here.  \n",
    "\n",
    "Though `DiscriminationMitigator` does not forbid it, we discourage users from reweighting multiple protected class features at the same time. The reason being that this attenuates the effect of the individual reweighted protected class feature. Each reweighted feature produces an *N* x 1 vector, so the preceding creates 3 counterfactual vectors, which are then averaged across for each person. Though the marginals may be weighted differently, this averaging may produce very similar predictions between `pop_wts` and `cust_wts` and should be hence avoided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom weights: {'c0': {0.0: 0.1, 1.0: 0.9}, 'c1': {0.0: 0.9, 1.0: 0.1}, 'z': {1.0: 0.9, 2.0: 0.02, 3.0: 0.04, 4.0: 0.04}} \n",
      "\n",
      "Dataframe of predictions: \n",
      "    unadj_pred  unif_wts   pop_wts  cust_wts\n",
      "0    6.661998  6.676531  6.654327  6.601067\n",
      "1    6.271629  6.202929  6.208470  6.155210\n",
      "2    5.450657  5.519357  5.515208  5.461948\n",
      "3    6.695879  6.654924  6.651216  6.597956\n",
      "4    6.580443  6.594976  6.572772  6.519513\n",
      "\n",
      "Statistical moments: \n",
      "        unadj_pred    unif_wts     pop_wts    cust_wts\n",
      "count  500.000000  500.000000  500.000000  500.000000\n",
      "mean     6.141468    6.141298    6.142123    6.088863\n",
      "std      0.559403    0.533642    0.526824    0.526824\n",
      "min      4.807796    4.876495    4.872347    4.819087\n",
      "25%      5.643587    5.679643    5.691246    5.637986\n",
      "50%      6.155550    6.154496    6.149170    6.095911\n",
      "75%      6.611161    6.593558    6.573488    6.520228\n",
      "max      7.422514    7.353815    7.359354    7.306095\n",
      "\n",
      "Correlation matrix of predictions: \n",
      "             unadj_pred  unif_wts   pop_wts  cust_wts\n",
      "unadj_pred    1.000000  0.998369  0.998899  0.998899\n",
      "unif_wts      0.998369  1.000000  0.999762  0.999762\n",
      "pop_wts       0.998899  0.999762  1.000000  1.000000\n",
      "cust_wts      0.998899  0.999762  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Custom weights:\", new_weights, \"\\n\")\n",
    "print(\"Dataframe of predictions: \\n\", ex4.reset_index(drop=True).head())\n",
    "print(\"\\nStatistical moments: \\n\", ex4.describe())\n",
    "print(\"\\nCorrelation matrix of predictions: \\n\", ex4.corr())"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
